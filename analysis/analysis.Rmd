---
title: "Analysis"
author: "Martin Hinz"
date: '2022-04-04'
output:
  html_document:
    keep_md: true
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preface

Running through the analysis can take a long time, especially the Bayesian model. We point out the long runtimes again at the appropriate place. Also, you might need at least 64 GB of RAM to avoid crashes due to missing memory.

## Loading the necessary libraries

For this analysis we need the following libraries:

```{r message=FALSE}
library(tidyverse)
library(ggdist)
library(reshape2)
library(here)
library(RcppRoll)
library(sf)
library(ggspatial)
library(vegan)
library(neotoma)
library(analogue)
library(rcarbon)
library(doParallel)
library(nimble)
library(coda)
library(MCMCvis)
library(fmcmc)
library(grid)
```

If you have not yet installed these libraries (and receive a corresponding error message), please install them!

For the aorist analysis we also use the package aoristAAR, which is not available on CRAN, but would have to be installed directly from GitHub:

```{r message=FALSE}
if(!require('devtools')) install.packages('devtools')
library(devtools)
install_github('ISAAKiel/aoristAAR')
require(aoristAAR)
```
Also, neotoma is not longer available on cran, and it has been replaced by neotoma2, so we had to replace it. That is why we have to load it again:

```{r}
if(!require('neotoma2')) devtools::install_github('NeotomaDB/neotoma2')
require(neotoma2)
```



```{r}
source(here("code/map_helpers.R"))
```

## Data collection

The data used in this analysis are the following proxies obtained from the following sources:

-   An openness index generated by a PCA of five pollen profiles in the Swiss Plateau from the neotoma database [@williams2018].

-   ^14^C sum calibration, data sets from the XRONOS database as well as from the unpublished PhD thesis of Julian Laabs [@laabs2019] and from the data collection of [@mart√≠nez-grau2021].

-   Sites from the database of the Cantonal Archaeology <!--# which! --> as the basis of an aoristic analysis

-   The number of dated fell phases at lakeshore settlements in the Three Lakes Region from the doctoral thesis by Julian Laabs [\@laabs2019].

### Openness index

We start with the openness index, as this will set the spatial framework for our analyses. The five pollen profiles are the following:

```{r}
pollen_locations <- read.csv(here("data","raw_data", "pollen_locations.csv"))
pollen_locations
```

We can also map these. To do this, we first transform the coordinate system into the Swiss national coordinate system, and then display them:

```{r}
pollen_locations.sf <- st_as_sf(pollen_locations, 
                                coords = c("lng", "lat"), crs = 4326)

base_map() + 
  geom_sf(data = pollen_locations.sf, inherit.aes = FALSE) +
  base_coord()
```

We download the data from neotoma:

```{r message=FALSE, warning=FALSE}
pollen_data <- get_downloads(pollen_locations$neotoma_id, all_data=T)
```

Then we load a few help scripts which can be found under "code/neotoma_helpers.R".

```{r}
source(here("code", "neotoma_helpers.R"))
```

Now we filter the data sets so that we only retain the pollen data that record "NISP".

```{r}
pollen_data.samples <- samples(pollen_data)

pollen_data.samples.filtered <- pollen_data.samples %>% subset(element == "pollen" & units == "NISP")
```

Then we convert the NISP into percentages:

```{r}
pollen_data_comp <- transform_to_percent(pollen_data.samples.filtered)
```

Furthermore, we need a (handwritten) concordance list for the taxa names:

```{r}
common_names <- read.csv(here("data", "raw_data", "species.csv"), row.names = 1)
common_names
```

With this concordance table we combine the species with respect to their genus:

```{r}
pollen_data_comp <- aggregate_by_joint_name(pollen_data_comp, common_names = common_names)
```

Now we can combine the individual pollen profiles into an overall table, where each row represents a sample:

```{r}
all_output <- pollen_data_comp
```

We restrict the data set to the Holocene so that we no longer have the pre-Holocene developments in the principal component analysis:

```{r}
timeframe <- c(-7000,-1000)
all_output <- subset(all_output, 1950-age > timeframe[1] & 1950-age < timeframe[2])
all_output[is.na(all_output)] <- 0
```

Then we extract the actual data:

```{r}
all_data <- all_output[, !(colnames(all_output) %in%  c("age", "sampleid", "sitename"))]
```

We then filter the data. Only those taxa are considered which are present in at least 1/3 of all data series:

```{r}
all_data <- all_data[ , colSums(all_data) != 0]
all_data <- all_data[,colSums(all_data>0)>(nrow(all_data)/3)]
```

In addition, only those with an average frequency greater than 0.1% are included:

```{r}
all_data <- all_data[,colMeans(all_data)>0.1]
```

However, we add the *cerealia* back in, regardless of their frequency:

```{r}
if(!("Cerealia" %in% colnames(all_data)))
{
  all_data <- data.frame(all_data, Cerealia = all_output[,"Cerealia"])  
}
```

These data can now be evaluated by means of a principal component analysis:

```{r}
res.pca <- rda(all_data, scale = TRUE)
biplot(res.pca, type="text")
```

This calculation already shows the familiar structure that we have the known open land indicators at one end of the spectrum and the forest species (tree pollen) at the other. However, the result is also influenced by a gradient which has nothing to do with the effect we are interested in: The immigration of beech. To eliminate this gradient, we use a partial RDA in which only the residuals for Fagus are evaluated:

```{r}
res.pca <- rda(all_data ~ Condition(Fagus), data=all_data, scale = TRUE)
biplot(res.pca, type="text")
```

From this principal component analysis, we now extract the first dimension as the openness index, and supplement it with the datings of the individual samples. In doing so, we ensure that open land indicators end up on the positive side of the result vector:

```{r}
orientation <- sign(scores(res.pca)$species["Cerealia",1])
res.open <- data.frame(scores(res.pca, choices = 1, display = "si")*orientation)
colnames(res.open) <- "score"
res.open$age <- all_output$age
res.open$site <- all_output$sitename
ggplot(res.open) + geom_line(aes(x = 1950-age, y = score, color = site))
```

The individual profiles show a very similar development over time. We now summarise the common trend and smooth it with a 50-year moving window. For this, however, we first have to interpolate shared points in time, which we do via a linear approximation:

```{r}

sw <- 50

full_range <- full_seq(res.open$age, 1)

res.open.interpolated <- by(res.open, res.open$site, function(x) approx(x$age,x$score, full_range)$y)

res.open.interpolated <- data.frame(do.call(cbind, res.open.interpolated))

res.open.interpolated$age <- full_range

res.open.interpolated <- melt(res.open.interpolated, id.vars=c("age"))

res.open.interpolated$site <- res.open.interpolated$variable

res.open.interpolated$variable <- NULL

res.open_smoothed <- res.open.interpolated %>% 
  group_by(site) %>%
  # RcppRoll::roll_mean() is written in C++ for speed 
  mutate(moving_mean = roll_mean(value, sw, fill = NA, na.rm = TRUE))

res.open_smoothed$value<-NULL
res.open_smoothed <- na.omit(res.open_smoothed)
```

Then we pick every 50th date:

```{r pollenproxy}
res.open_smoothed <- res.open_smoothed[res.open_smoothed$age %in%
seq(from=1000,
    to=10000, by=50),]
pollenproxy_plot <- ggplot(res.open_smoothed,aes(x=1950-age,y=moving_mean)) +
  geom_point(aes(color=site), alpha =.5) +
  geom_smooth(span = 0.05, color = "black") +
  scale_x_continuous(breaks=seq(timeframe[1],timeframe[2],200), limits = timeframe) +
  labs(y = "Openness score [PCA]",x = "Date",colour = "Sites") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
pollenproxy_plot
```
We save this plot for later use:

```{r}
ggsave(here("figures","pollenproxy.pdf"),
       plot = pollenproxy_plot, width = 21, height = 29.7/2, units = "cm")
```

Finally, we calculate the average of all profiles as the final supra-regional openness index.

```{r}
res.open_final <- res.open_smoothed %>%
  group_by(age) %>%
  mutate(mean=mean(moving_mean),
         sd = sd(moving_mean)) %>% 
  select(age,mean,sd) %>%
  unique()  %>% arrange(desc(age))

ggplot(res.open_final,aes(x=1950-age)) +
  geom_line(aes(y=mean)) +
  geom_segment(aes(y=mean-sd, yend =mean+sd, xend=1950-age), alpha = .25)
```

### ^14^C Summenkalibration

The data set for the 14C sum calibration is composed of the above-mentioned sources. This data consists of 1135 dates from 246 sites.

```{r}
c14 <- read.csv(here("data","raw_data", "14c_swiss_plateau.csv"))
```

We can show this on the map:

```{r}
c14.sf <- c14 |>
  distinct(site, lat, lng) |>
  st_as_sf(coords = c("lng", "lat"),  crs = 4326)

base_map() +
  layer_spatial(c14.sf) +
  base_coord()
```

Next, we calibrate the 14C data using rcarbon: 

```{r}
c14.calibrated <- calibrate(x = c14$bp, errors = c14$std, verbose = F)
```

Subsequently, we perform the sum calibration, whereby we perform a binning at the level of the sites:

```{r}
sumcal <- spd(c14.calibrated, timeRange = 1950-timeframe, bins = c14$site, runm = sw, verbose = F)
```

Below is the visualisation of the sum calibration:

```{r}
plot(1950-sumcal$grid$calBP, sumcal$grid$PrDens,type="l")
```

Finally, we extract the values every 50 years:

```{r}
res.sumcal <- sumcal$grid[sumcal$grid$calBP %in%
seq(from=1000,
    to=10000, by=50),]
```

### Aoristic Site Count

The third proxy is the aoristic sum of the number of sites from the cantonal archaeology database. For these, we have obscured the coordinates to avoid the well-known problems that can arise from clear site coordinates.

```{r}
sites <- read.csv(here("data","raw_data", "site_data_fuzzy.csv"))
```

We can depict this on the map:

```{r}
sites.sf <- st_as_sf(sites,
                   coords = c("LV03_Rechtswert_fuzzy", "LV03_Hochwert_fuzzy"),
                   crs = 21781)

base_map() + 
  layer_spatial(sites.sf) +
  base_coord()
```

The dates come in two categories, rough and very rough We use the rough datings where they are available. Where they are not, we fall back on the very rough dates. But first we load the concordance table:

```{r}

chrono_rough <- read.csv(here("data","raw_data", "chrono_rough.csv"),
                         row.names = 1)

chrono_vrough <- read.csv(here("data","raw_data", "chrono_very_rough.csv"),
                          row.names = 1)
```

Next, we prepare the beginning and end as a column:

```{r}
sites$begin <- NA
sites$end <- NA
```

And then we first fill these columns with the data from the very rough dating:

```{r}
for(i in 1:nrow(chrono_vrough)){
  this_phase <- chrono_vrough[i,]
  sites$begin[sites$chrono_very_rough == this_phase[,1]] <- this_phase$dat_begin
  sites$end[sites$chrono_very_rough == this_phase[,1]] <- this_phase$dat_end
}
```

Where available, we overwrite the very rough dating with the rough dating:

```{r}
for(i in 1:nrow(chrono_rough)){
  this_phase <- chrono_rough[i,]
  sites$begin[sites$chrono_rough == this_phase[,1]] <- this_phase$dat_begin
  sites$end[sites$chrono_rough == this_phase[,1]] <- this_phase$dat_end
}
```

However, this does not allow for a very precise dating:

```{r}
table(sites$begin)
```

Nevertheless, we next use the aorist sum as a mapping of the expected value for simultaneously existing settlements:

```{r}
res.aorist <- aorist(sites, from = "begin", to = "end", method = "weight")

res.aorist$age <- 1950-res.aorist$date
res.aorist <- subset(res.aorist, date >= timeframe[1] & date <= timeframe[2])
```

Again, the curve of the proxy is plotted here also:

```{r}
plot(res.aorist$date, res.aorist$sum,type="l", main = paste0("n=", nrow(sites)))
```

Once more, we extract the values every 50 years:

```{r}
res.aorist <- res.aorist[res.aorist$age %in%
seq(from=1000,
    to=10000, by=50),]
```

### Dendro Dated Settlements

This year-by-year dataset maps the number of lakeshore settlements in the Three Lakes Region. Details can be found in the publication by [@laabs2019]. To match the resolution of the other datasets, this dataset is also sampled at 50-year intervals, and smoothed accordingly beforehand using a moving average.

First we load the data:

```{r}
lss <- read.csv(here("data","raw_data", "dendro_years.csv"))
```

Then we complete the data set so that it covers the entire period under consideration:

```{r}
lss <- rbind(data.frame(date=-7050:min(lss$date)-1,n_settlements=0),
             lss,
             data.frame(date=max(lss$date)+1:-1,n_settlements=0))
```

Finally, we run a moving average over it with the same window as for the other data sets:

```{r}
res.lss <- lss %>% 
  mutate(moving_mean = roll_mean(n_settlements, sw, fill = NA, na.rm = TRUE))
```

Here is yet another plot of the resulting curve:

```{r}
plot(res.lss$date, res.lss$moving_mean,type="l")
```

Picking the data points every 50 years again:

```{r}
res.lss <- res.lss[res.lss$date %in% seq(from=timeframe[1],
                                         to=timeframe[2], by=50),
        ]
res.lss$age <- 1950-res.lss$date
```

### Combining the proxies

Thus we have all proxies available, and can display them (scaled) all together:

```{r}
ggplot() +
  geom_line(data = res.open_final, aes(x = 1950-age, y = scale(mean)), col="darkgreen", alpha = 0.75) +
  geom_line(data = res.sumcal, aes(x = 1950-calBP, y = scale(PrDens)), col="red", alpha = 0.75) +
  geom_line(data = res.aorist, aes(x = date, y = scale(sum)), col="gray", alpha = 0.75) + 
  geom_line(data = res.lss, aes(x = date, y = scale(moving_mean)), col="brown", alpha = 0.75) +
  scale_colour_manual(values = c("darkgreen", "red", "gray", "brown"),
                      labels = c("Openness Indicator", "Sum Calibration", "Aoristic Sum", "Dendro Dated Settlements")
                      )
```

We merge the dataset and restrict it to our observation window:

```{r}
all_proxies <- merge(res.sumcal, res.open_final, by.x = "calBP", by.y = "age") %>%
  merge(res.aorist, by.x = "calBP", by.y = "age") %>%
  merge(res.lss, by.x = "calBP", by.y = "age") %>%
  select(calBP, PrDens, mean, sum, moving_mean) %>%
  rename(age = calBP, sumcal = PrDens, openness = mean, aoristic_sum = sum, dendro = moving_mean) %>%
  dplyr::filter(age < 8000 & age > 3000)

all_proxies
```

We might like to store the result for the actual analysis:

```{r}
write.csv(all_proxies, here("data","preprocessed_data", "all_proxies.csv"))
```

## Transforming the Data

If the data has already been created in a previous run, we can also reload it directly at this point:

```{r}
all_proxies <- read.csv(here("data","preprocessed_data", "all_proxies.csv"),
                        row.names = 1)
```

First we reverse the order of the data so that it is sorted from oldest to youngest:

```{r}
all_proxies <- all_proxies %>% arrange(desc(age))
```

Since the data influence the model in their respective change from the state t -\> t-1, we first normalise them by a z-transformation, and then calculate the individual difference to the predecessor. The first row is set to 0.

```{r}
all_proxies_diff <- all_proxies
all_proxies_diff[,2:5] <- all_proxies_diff[,2:5] %>% scale() %>% diff() %>% rbind(0,.)
```

We can also visualise these deltas of the proxies:

```{r}
all_proxies_diff %>% pivot_longer(2:5) %>% ggplot(aes(x = age, y = value)) + geom_line() + facet_wrap(. ~ name)
```


Furthermore, it has proven practical, at least in the development phase of the model, to be able to restrict the data set in order to quickly try out responses of the model. Each data point is included in the model as a separate parameter, and it therefore does not scale linearly with the number of data points. It can also be helpful for reproducing the analysis. To do this, we define a reduction factor that determines what proportion of the data is passed to the model and reduce the data set accordingly.

```{r}
redux_factor <- 1
redux_nrow <- round(nrow(all_proxies_diff) * redux_factor)
redux_seq <- round(seq(from=1,to=nrow(all_proxies_diff), length.out = redux_nrow))

model_data <- all_proxies_diff[redux_seq,]
```

Furthermore, the model contains two constraints for the maximum growth rate. These help to speed up convergence. They are given as data and must be set to 1 for the constraint to be processed.

```{r}
model_data$constraint_lambda_lower <- 1
model_data$constraint_lambda_upper <- 1
```

## The Model

The actual model consists of a nimble code block that describes the relationships and sets the priors for the parameters:

```{r}
model_code <- nimbleCode( {
  # ---- Process Model ----
  
  # Estimate the initial state vector of population abundances
  nEnd ~ dnorm(z[nYears] * Meapize / AreaSwissPlateau, sd=0.5)

  # Autoregressive process for remaining years
  for(t in 2:(nYears)) {
    # The actual number of sites per year
    z[t] ~ dpois(lambda[t])
    
    # limiting the change to a maximum value, estimated in the model
    constraint_lambda_lower[t] ~ dconstraint(
      z[t]/z[t-1] < (max_growth_rate + 1)
      )
    constraint_lambda_upper[t] ~ dconstraint(
      z[t-1]/z[t] < (max_growth_rate + 1)
      )
  }
  
  # ---- Observational Model ----

  # For all but the first year
  for(t in 2:(nYears)) {
    # lambda depends on the number of sites at the previous year, plus
    # changes in relation to the proxies
    log(lambda[t]) <- log(z[t-1]) + (
      beta[1] * sumcal[t]  +
        beta[2] * openness[t] +
        beta[3] * aoristic_sum[t] + 
        beta[4] * dendro[t]
      )
  }
  # ---- Priors ----
  # Relevance of the proxies is estimated as Dirichlet distribution
  beta[1:4] ~ ddirch(alpha[1:4])
  
  # The parameters for the Dirichlet distribution have a weakly informative prior
  for(j in 1:4) {
    alpha[j] ~ dlnorm(mu_alpha[j],sdlog=a_alpha[j])
    a_alpha[j] ~ dexp(1)
    mu_alpha[j] ~ dlnorm(1,sdlog=0.1)
  }
  
  # The maximum growth rate has a prior gamma distributed between 0 and 1
  # by adding 1 in the process model, this becomes 1-2[
  max_growth_rate ~ dgamma(shape = 5, scale=0.05)

  # The mean site size
  MeanSiteSize ~ dpois(50)
    
  # ---- transformed data ----
  
  # Population density and total population as function of site number
  PopDens[1:(nYears)] <- PopTotal[1:(nYears)] / AreaSwissPlateau
  PopTotal[1:nYears] <- z[1:nYears] * MeanSiteSize
})
```

## Run Preparation

A few things still need to be specified for the model: constants and initial values. Let's start with the constants:

```{r}
model_constants <- list(
  # an estimation for the population density at the end of the modeled period
  nEnd = 5, 
  # the number of time slices
  nYears = redux_nrow,
  # the area of the swiss plateau for the transformation
  # of population density to Total Population
  AreaSwissPlateau = 12649
  )
```

Finally, it helps for convergence to give meaningful starting values for the chains:

```{r}

# Calculation of an exponential increase of the factor 10
r <- log(1 - (10^(1/model_constants$nYears) - 1))
model_inits <- list(
  lambda = rep(r,redux_nrow),
  PopDens = rep(model_constants$nEnd,redux_nrow),
  z = rep(50,redux_nrow)
)
```

Furthermore, we set the parameters for the runs until convergence:

```{r}
batches <- 100000 # run in batches of this length
thinning <- 10 # save only each ... of the chains
```

Finally, we have to define which parameters (apart from the top-level stochastic nodes of the model) are to be observed and stored. We also include the population density as a core parameter and the relative proportion of proxies beta:

```{r}
model_monitors <- c("PopDens", "beta")
```

## Parallelisation

The model runs significantly faster if we use multiple cores in parallel. Nimble itself does not have an option for parallelisation, so we use functions provided by the package `do_parallel` for this. The computer used for the analysis has 4 cores, you may need to adjust these values for your computer.

```{r}

# Spare 4 cores from the calculation
ncore <- detectCores() - 4
ncore <- max(ncore, 1)

# Register the cluster
cl <- makeCluster(ncore)
registerDoParallel(cl)

# Export common values for the cluster
clusterExport(cl,
              c("model_code",
                "model_inits",
                "model_data",
                "model_constants",
                "model_monitors",
                "batches",
                "thinning"
                )
              )

# Set random seeds
for (j in seq_along(cl)) {
  set.seed(j)
}
```

## First batch

In the first run, the clusters are set up and the models are instantiated and started within the clusters.

```{r}
start_time <- Sys.time()

mcmcSamples <- clusterEvalQ(cl, {
  
  # Load necessary libraries
  library(nimble)
  library(coda)
  
  # initiate the model with the parameters and dates
  model <- nimbleModel(code=model_code,
                       data=model_data,
                       constants = model_constants,
                       inits = model_inits)
  
  # Compile the model
  Cmodel <- compileNimble(model)
  
  # Configure the model
  modelConf <- configureMCMC(model, thin = thinning)
  
  # Add the monitor(s)
  modelConf$addMonitors(model_monitors)
  
  # Build the mcmc
  modelMCMC <- buildMCMC(modelConf)
  
  # Compile the final model
  CmodelMCMC <- compileNimble(modelMCMC, project = model)
  
  # Run the model for the number of iterations specified in batches
  CmodelMCMC$run(batches, reset = FALSE)
  return(as.mcmc(as.matrix(CmodelMCMC$mvSamples)))
})


end_time <- Sys.time()
end_time - start_time
```

At the end of the 1st run, the model is instantiated in the individual cluster partitions, possibly already converged. But we now check the convergence in continued runs.

```{r}
# Initialize with non-convergence
converged <- F

# minimum psrf until the model is considered converged
min_convergence <- 1.1

start_time <- Sys.time()

# As long as not converged
while (!converged) {
  
  # run the model for another batch, with resetting the values (burn-in)
  mcmcSamples <- clusterEvalQ(cl, {
    CmodelMCMC$run(batches, reset = FALSE, resetMV = TRUE)
    return(as.mcmc(as.matrix(CmodelMCMC$mvSamples)))
  })

  # convert the resulting list to an mcmc.list object
  mcmcSamples <- as.mcmc.list(mcmcSamples)

  # Check the psrf values from the Gelman and Rubin's convergence diagnostic
  gelman <- gelman.diag(mcmcSamples, multivariate = F)
  psrf <- gelman$psrf[,1]

  # The model is considered converged when
  # all psrf are lower than the minimum convergence criterion
  converged <- all(psrf<min_convergence, na.rm=T)
  
  # optional: create an mcmc traceplot for visual inspection
  #MCMCtrace(window(mcmcSamples, start = (batches/thinning)-min((batches/thinning),5000)), params = "PopDens", pdf = F)
}

end_time <- Sys.time()
end_time - start_time
```

Once the model has converged, we can look at the result of estimating the population density based on the number of settlements. For this we extract the mean and 95% highest posterior density interval:

```{r}
result <- MCMCsummary(mcmcSamples, HPD = T, params = "PopDens")

plotmax <- max(result$`95%_HPDU`)

plot(1950-model_data$age, result$mean, type="l", ylim = c(0,plotmax))
lines(1950-model_data$age, result$`95%_HPDL`, lty=2)
lines(1950-model_data$age, result$`95%_HPDU`, lty=2)
```

This converged run is sufficient to check some initial parameter values. For example, we can see how the proportional importance of the proxies is distributed:

```{r}
MCMCchains(mcmcSamples, params = "beta") %>%
  as.data.frame %>%
  rename(sumcal = 'beta[1]', openness = 'beta[2]', aorist = 'beta[3]', dendro = 'beta[4]') %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value, y = name)) + stat_halfeye()
```

The model trusts mainly the openness index, the aoristic sum and the dendro data play a subordinate role. The sum calibration has a higher weight. However, the shape of the distributions also indicates that we may have too few effective samples to make a reliable statement about the highest posterior density intervals. Let's take a look at the effective samples:

```{r fig.width=5,fig.height=12}
MCMCsummary(mcmcSamples) %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  ggplot() + geom_bar(aes(y=n.eff, x=rowname), stat = "identity") + coord_flip()
```

Very few parameters have already reached the effective sample size of 10000 suggested for a reliable estimate. Therefore, we extend the run until this number is reached.

## Finale Run after Convergence

The final run after convergence to get more than the min 10000 effective samples for each parameter takes quite some time. On my machine, it took nearly 5 hours. Therefore, I add a switch here for not doing this analysis, but feel free to proceed yourself, if you like:

```{r}
do_final_run = F
```

First we set our new meta-parameters:

```{r}
# Starting with not enough samples
enough_samples <- F

# We want at least 10000 effective samples
min_eff_samples <- 10000

# We prolongue the batches for more time efficient sampling
sample_batches <- 1000000
```

Then we export the new batch length to the clusters:

```{r}
clusterExport(cl, c("sample_batches"))
```

In order to work memory-efficiently (the model can need a lot of memory quickly!), we only allow the current samples to be used within the cluster instances and collect them outside the clusters in a variable:

```{r}
start_time <- Sys.time()
while (!enough_samples & do_final_run) {
  
  gc(verbose=F) # Garbage collector for more RAM

  # Start sampling  
  mcmcSamples <- clusterEvalQ(cl, {
    CmodelMCMC$run(sample_batches, reset = FALSE, resetMV = TRUE)
    return(as.mcmc(as.matrix(CmodelMCMC$mvSamples)))
  })
  
  mcmcSamples <- as.mcmc.list(mcmcSamples)
  
  # create or append values to a collector variable
  if(!exists("collector")){
    collector <- mcmcSamples
  } else {
    collector <- append_chains(collector, mcmcSamples)
  }
  
  # calculate effective sample size iteratively over the chains
  # and then sum the individual values
  # more RAM efficient than the `MCMCsummary` implementation
  mcmcsummary <- lapply(collector, effectiveSize)
  ness <- Reduce(`+`, mcmcsummary)
  
  # we have enough samples, when all parameters have more than the minimum ESS
  enough_samples <- all(ness >  min_eff_samples)

  # just in case: store the result in a RDS file for later inspection  
  saveRDS(collector, file = "mcmc_collector.RDS")
}

end_time <- Sys.time()
end_time - start_time
```

When the model has finished running, we should stop the clusters:

```{r}
stopCluster(cl)
```

If the final run above was skipped, but you have already the results from a previous run, you can load this at this stage:

```{r}
collector <- readRDS("mcmc_collector.RDS")
```


For comparison with the priors, we simulate from these, first for beta:

```{r}
n_prior <- 100000
a_alpha <- mu_alpha <- alpha <- list()

for(j in 1:4) {
a_alpha[[j]] <- rexp(n_prior, 1)
mu_alpha[[j]] <- rlnorm(n_prior, 1,sdlog=0.1)
alpha[[j]] <- rlnorm(n_prior, mu_alpha[[j]],sdlog=a_alpha[[j]])
}

beta_mat <- matrix(ncol = 4, nrow = n_prior)
for(i in 1:n_prior){
 beta_mat[i,] <- rdirch(1, c(alpha[[1]][i], alpha[[2]][i], alpha[[3]][i], alpha[[4]][i]))
}
MCMCtrace(collector, params = "beta", priors = beta_mat, pdf = F, ind = T, Rhat = T)
```

And then for lambda and z (respectively derived for PopDens):

```{r}
z <- matrix(ncol = model_constants$nYears, nrow = n_prior)
MeanSiteSize <- rpois(n_prior, 50)

nEnd <- rnorm(n_prior, model_constants$nEnd, sd=0.5)

z[,model_constants$nYears] <- rpois(n_prior,nEnd / MeanSiteSize * model_constants$AreaSwissPlateau)

for(i in (model_constants$nYears-1):1) {
  z[,i] <- rpois(n_prior, z[,i+1])
}

PopDens <- z * MeanSiteSize / model_constants$AreaSwissPlateau

MCMCtrace(collector, params = "PopDens", priors = PopDens, pdf = F, ind = T, Rhat = T)
gc(verbose=F)
```

Finally, we can plot the result in comparison to the input data, superimposing them (scaled) on the estimation result in their original form rather than as difference data. To do this, we first extract mean and HPDI from the posterior samples for all parameters:

```{r}

all_proxies_orig <- read.csv(here("data","preprocessed_data", "all_proxies.csv"), row.names = 1)

# MCMCvis methods are rather memory intensive. We implement the summary from scratch
all_chains <- as.matrix(collector)

rm(collector)

gc(verbose=F)

params <- colnames(all_chains)

this_mcmc_summary <- data.frame()

for(this_column in colnames(all_chains)) {
   this_mcmc_summary[this_column, "mean"] <- mean(all_chains[,this_column])
   hdpi_i <- coda::HPDinterval(as.mcmc(all_chains[,this_column]))
   this_mcmc_summary[this_column, "hpdi_l"] <- hdpi_i[1]
   this_mcmc_summary[this_column, "hpdi_u"] <- hdpi_i[2]
}

gc(verbose=F)
```

Then we obtain the population density estimates from these, and save them for further use:

```{r}
PopDens_summary <- this_mcmc_summary[grep("^PopDens", params),]
PopDens_summary$age <- rev(all_proxies_orig$age)
write.csv(PopDens_summary, 
          here("data", "preprocessed_data", "popdens_summary.csv"))
```

Finally, compile the plot:

```{r}
all_proxies_orig <- read.csv(here("data","preprocessed_data", "all_proxies.csv"), 
                             row.names = 1)

proxies_and_result <- rbind(melt(all_proxies_orig, id.vars = "age"),
                            melt(PopDens_summary[,2:5], id.vars = "age")
)

variable_full_name = c(
      sumcal = "Sum Calibration",
      openness = "Openness",
      aoristic_sum = "Aoristic Sum",
      dendro = "Dendro Dated Settlements",
      mean = "Estimation"
      )

popdens_plot <- ggplot() +
  geom_line(data = subset(
    proxies_and_result,
    variable %in% c("mean",
                    "openness",
                    "sumcal",
                    "dendro",
                    "aoristic_sum")
    ),
    aes(x = (1950-age) * -1, y = value, color = variable)) +
    geom_ribbon(data = subset(
      proxies_and_result,
      variable %in% c("mean")
      ),
      aes(x = (1950-PopDens_summary$age) * -1,
          ymin = PopDens_summary$hpdi_l,
          ymax = PopDens_summary$hpdi_u),
      fill = "grey70", alpha = 0.5) +
  facet_wrap(. ~ variable,
             scales="free_y",
             ncol = 1,
             labeller = labeller(variable = variable_full_name)) +
  theme_minimal() + scale_x_reverse() + theme(legend.position="false") +
  ylab("Scaled values for Proxies,\n P/km¬≤ for Estimation") +
  xlab("cal BCE") + 
  scale_colour_manual(
    values = c("sumcal" = "red",
      "openness" = "darkgreen",
               "Aoristic Sum" = "gray",
               "dendro" = "brown",
               "mean" = "black")
    )

popdens_plot_adjusted <- ggplotGrob(popdens_plot)

popdens_plot_adjusted$heights[[28]] <- unit(4, 'null')

grid.newpage()
grid.draw(popdens_plot_adjusted)
```

Save as result image to the figures folder.

```{r}
ggsave(here("figures", "popdens_estimation.pdf"),
       plot = popdens_plot_adjusted, 
       width = 21, height = 29.7/2, units = "cm")
```

And than, the final estimate of the influence of the different proxies on the result:

```{r}
gc(verbose = F)

beta_plot <- all_chains[,grep("^beta\\[", params)] %>%
  as.data.frame %>%
  rename(sumcal = 'beta[1]', openness = 'beta[2]', aorist = 'beta[3]', dendro = 'beta[4]') %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value, y = name)) +
  stat_halfeye() + xlab("Percentage Influence") + ylab("Proxy") + theme_minimal()
beta_plot
```

Also save as result image.

```{r}
ggsave(here("figures","beta_estimation.pdf"),
       plot = beta_plot, width = 21, height = 29.7/2, units = "cm")
```
Also interesting is the variation coefficient from the estimation of the Population density:

```{r}
gc(verbose = F)

var_popdens <- data.frame(var_coeff = apply(all_chains[,2:100], 2, function(x) sd(x)/mean(x)),
                          age = rev(all_proxies_orig$age))

coeff <- 0.1

popvar_plot <- ggplot(var_popdens) +
  geom_line(aes(x = (1950-age) * -1, y = var_coeff/coeff, col="Variation Coefficient"), alpha = 0.5) +
  geom_line(data = PopDens_summary,
            aes(x = rev((1950-all_proxies_orig$age)*-1), y=mean, col = "Estimation Result")) + 
  geom_line(data = PopDens_summary,
            aes(x = rev((1950-all_proxies_orig$age)*-1), y=hpdi_l, col = "Estimation Result"), lty=2) + 
  geom_line(data = PopDens_summary,
            aes(x = rev((1950-all_proxies_orig$age)*-1), y=hpdi_u, col = "Estimation Result"), lty=2) +
  theme_minimal() + scale_x_reverse() + 
  scale_colour_manual(
    values = c(
               "Variation Coefficient" = "red",
               "Estimation" = "black"), name = "") +
  theme(legend.position="bottom") +
  xlab("cal BCE") +
  
  scale_y_continuous(
    
    # Features of the first axis
    name = "Scaled values for Proxies,\n P/km¬≤ for Estimation",
    
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*coeff, name="Variation Coefficient")
  ) +

  theme(
    axis.title.y.right = element_text(color = "red"),
    axis.text.y.right = element_text(color = "red"),
     axis.line.y.right = element_line(color = "red"), 
     axis.ticks.y.right = element_line(color = "red")
  )


popvar_plot
```

Also save as result image.

```{r}
ggsave(here("figures","popvar_plot.pdf"),
       plot = popvar_plot, width = 21, height = 29.7/2, units = "cm")
```

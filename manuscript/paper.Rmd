---
title: "Bayesian inference of prehistoric demography from multiple proxies: a case study from the Swiss plateau"
author:
  - Martin Hinz:
      email: martin.hinz@iaw.unibe.ch
      institute: [IAW, OCCR]
      correspondence: true
  - Joe Roe:
      email: joe@joeroe.io
      institute: [IAW, OCCR]
      correspondence: false
  - Julian Laabs:
      email: julian.laabs@ufg.uni-kiel.de
      institute: [SFB1266]
      correspondence: false
  - Caroline Heitz:
      email: caroline.heitz@arch.ox.ac.uk
      institute: [IAW, OCCR, OX]
      correspondence: false
  - Jan Kolář:
      email: jan.kolar@ibot.cas.cz
      institute: [IBOT, IAMFAMU]
      correspondence: false
institute:
  - IAW: Institute of Archaeological Sciences, University of Bern
  - OCCR: Oeschger Centre for Climate Change Research, University of Bern
  - SFB1266: CRC 1266 - Scales of Transformation, University of Kiel
  - OX: Associated Researcher, School of Archaeology, University of Oxford
  - IBOT: Department of Vegetation Ecology, Institute of Botany of the Czech Academy of Sciences
  - IAMFAMU: Institute of Archaeology and Museology, Faculty of Arts, Masaryk University
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::word_document2:
    fig_caption: yes
    reference_docx: "../templates/template.docx" # Insert path for the DOCX file
    pandoc_args:
    - --lua-filter=../templates/scholarly-metadata.lua
    - --lua-filter=../templates/author-info-blocks.lua
    - --lua-filter=../templates/pagebreak.lua
  bookdown::pdf_document2:
    toc: no
    pandoc_args:
    - --lua-filter=../templates/scholarly-metadata.lua
    - --lua-filter=../templates/author-info-blocks.lua
    - --lua-filter=../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  In demography in general, the last decade has triggered an upswing in the application of Bayesian methods, so that a Bayesian demography has been announced. Bayesian demography is currently at the forefront of methodological developments in this area, but has reached such maturity that the United Nations has been using it since 2015 for population forecasts and projections. The Bayesian approach offers the possibility of combining heterogeneous data and at the same time qualifying them in terms of uncertainty and credibility. This is precisely where it becomes very interesting for archaeological data, since they are mostly inaccurate, biased, sparse, simplified and often based on simplistic assumptions.
  In this contribution, a Bayesian hierarchical model, based on a Poisson Regression and a state space approach is applied to the field of archaeological (prehistoric) population size and density estimation. This represents a pilot study for the reconstruction of demographic developments in prehistory based on data assimilation with an underlying process model and a number of different proxies, which are combined and result in a more reliable estimation than a single proxy could deliver.
  In order to understand human-environment relationships in the past and the dynamics of socio-ecological systems, it is essential to have a robust estimate of population sizes<!-- JR: Could reference Shennan 2000 and/or Kintigh 2014 here? MH: I would not like to have any citation in the abstract. What do you think?-->. Size and density of population are crucial factors for the reconstruction of group sizes, the character and range of political institutions for social organization, the constraints and possibilities of economic practice, the number of people available for collective activities, for economic and social exchange systems, and the creation and maintenance of collective identities. And, of course, the human influcence on the environment. Most estimates of population sizes currently work on limited spatial scales or use proxies that hardly allow for absolute numbers (e.g. sum calibration). The figures in the archaeological discourses on population density in general currently come from vague sources and are often decades old.
keywords: |
  Bayesian demographic modelling; Multiproxy; Reconstruction of past settlement dynamics
highlights: |
  - We provide a estimation based on the combination of different proxies.
  - We estimate population development in terms of absolute settlement densities.
  - We are developing a completely new method for the integration of heterogeneous information.
---

Keywords: `r rmarkdown::metadata$keywords`

Highlights: `r rmarkdown::metadata$highlights`

```{r setup, echo = FALSE, message = FALSE, warning=FALSE, results='hide'}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/",
  dpi = 300
)

library(here)
library(ggplot2)
library(ggmap)
library(cowplot)
library(sp)
library(sf)
library(ggsn)
library(rnaturalearth)
library(ggrepel)

switzerland <- ne_countries(country = "Switzerland")
bbox_ch <- bbox(switzerland) %>% as.vector()

switzerland_bb <- st_as_sfc(st_bbox(switzerland))

swiss_background <- get_stamenmap( bbox = bbox_ch, maptype = "terrain-background", color="bw", zoom = 8)

rivers10 <- ne_download(scale = "large", type = 'rivers_lake_centerlines', category = 'physical') %>% st_as_sf()

rivers10eu <- ne_download(scale = "large", type = 'rivers_europe', category = 'physical') %>% st_as_sf()

lakes10 <- ne_download(scale = "large", type = 'lakes', category = 'physical') %>% st_as_sf()

lakes10eu <- ne_download(scale = "large", type = 'lakes_europe', category = 'physical') %>% st_as_sf()

my_basemap <- ggmap(swiss_background, darken = c(0.6, "white")) + coord_sf(default_crs = st_crs(4326)) + geom_sf(data = rivers10, color="lightblue4", inherit.aes = F)  + geom_sf(data = rivers10eu, color="lightblue4", inherit.aes = F)  + geom_sf(data = lakes10, fill="lightblue4", color=NA, inherit.aes = F)  + geom_sf(data = lakes10eu, fill="lightblue4", inherit.aes = F, color=NA)

my_scalebar <- scalebar(transform = T, dist_unit = "km", dist=50, model = "WGS84", st.dist = 0.025, st.size = 3, x.min = bbox_ch[1], x.max = bbox_ch[3]-0.2, y.min = bbox_ch[2]+0.15, y.max = bbox_ch[4], border.size = 0.5)

europe <- c(left = -12, bottom = 35, right = 30, top = 63) %>% as.vector() %>% +0.01

europe_overview <- get_stamenmap( bbox = europe, maptype = "toner-background", color="bw", zoom = 3)

europe_overview_map <- ggmap(europe_overview, darken = c(0.6, "white"))+ coord_sf(default_crs = st_crs(4326))

fc <- sf::st_read("../data/fixed_online_data/BiogeographischeRegionen.gdb", layer = "N2020_Revision_BiogeoRegion")
fc <- st_transform(fc, 4326)
fc <- fc[fc$RegionNummer==2,] %>% st_union()

plot_path <- normalizePath(
  file.path(
    here(), "figures"
    )
  )
```

<!--
[JR] suggested alternative titles:
* A Bayesian hierarchical model of prehistoric population dynamics on the Swiss Plateau
* Multi-proxy estimation of prehistoric population dynamics on the Swiss Plateau using Bayesian hierarchical modelling
* Combining multiple proxies to estimate prehistoric population dynamics on the Swiss Plateau using Bayesian hierarchical modelling
* A high-resolution estimate of prehistoric population dynamics on the Swiss Plateau using multi-proxy Bayesian hierarchical modelling
* Prepend "Beyond summed radiocarbon:" to any of the above
* Bayesian inference of prehistoric demography from multiple proxies: a case study from the Swiss plateau
-->

# Introduction

## Prehistoric demography as object of research

The study of population size and population density as important demographic factors has a long history in archaeology. Although questions of population extent and distribution have been dominant fields since the beginning of archaeological research, the lack of access to this information, data on it, or methodological tools for the use of proxies has long made the statements made on this topic for prehistoric times vague and superficial [@hassan_demographic_1981], and they were often based on uncritical transfer of ethnographic parallels.

Gordon Childe [-@childe_man_1936] was one of the first to stress the importance of estimating the density and characteristics of prehistoric populations and sought to examine the role of human populations in the course of cultural evolution [@hassan_demographic_1981].

Attempts to estimate the size and density of prehistoric populations from archaeological remains were made by Hack [-@hack_changing_1942], Colton [-@colton_prehistoric_1949], Frankfort [-@frankfort_town_1950] and Cook [-@cook_reconsideration_1946]. Cook and Heizer (1965, 1968) and Naroll (1962) provided rules derived from ethnographic contexts for estimating the size of prehistoric populations. Cook and Heizer (1966) made the first attempt to estimate the population growth rate during the Neolithic [@hassan_demographic_1981].

At the end of the 1960s, however, with the emergence of the processual archaeology, such investigations became increasingly popular. It was the increased focus on human-environment relations paradigm that increased the emphasis on population studies, especially those dealing with population ecology [@hassan_demographic_1981].

After research interest turned increasingly to other aspects with the emergence of post-process trends, a strong boom in the study of population sizes can be observed especially since the 2010s [@riede_climate_2009 and others in that special issue of Human Biology, or even earlier, @shennan_population_2000]. This is probably not at least due to the new shift towards the study of human-environment relationships, for which an assessment of the size of human populations is indispensable. This is true especially for an evaluation of the human impact on the natural environment. Another reason is certainly the emergence of the 'dates as data' approach [@rick_dates_1987], which only really took off with the publications around the London UCL group [@shennan_regional_2013], but also others []. Moreover, Kintigh et al. [-@kintigh_grand_2014] listed human influence, dominance and population size and growth as one of the key elements where methodological improvements are necessary in the article on the Grand Challenges for archaeology, in the area of 'human-environment interactions'. Methodology continues to evolve, but we believe that essential problems underlying summation calibration cannot be solved by methodological refinements in this area alone [@french_manifesto_2021, @schmidt_approaching_2021].

## Issues associated with existing approaches

Müller and Diachenko [@muller_tracing_2019] have compiled a list of currently used approaches and proxies for estimating prehistoric population sizes. These can be roughly divided into ethnographic parallels, ecologic-economic deductive estimates, and the counting and spatial interpolation of archaeological features (settlements, houses, individual find types). From this short literature review three basic problems are common to all these approaches and the studies based on them:

1.  Reliance on single line of evidence: Most investigations are using only one proxy. Although multiproxy approaches exist, in such cases the individual proxies only serve to support each other or the proxy used in the main case. There is no combination of results to increase the accuracy of each of the inaccurate and biased proxies.
2.  Uncertainty in measurements: In most cases, individual curves are presented as estimates, but the size of the error bars for these estimates is almost never apparent. This means, that the uncertainty of evidence and derived quantities is not adequately reflected.
3.  Lack of transfer functions: What is meant here is that they take little account of the inherent uncertainties of the proxies themselves, for lack of an evaluative framework, and that transfer functions from the proxy to the actual size to be observed, i.e. population size or density, are hardly ever evaluated more intensively. That means, that the nature of the archaeological data is taken into account only insufficiently. In the best case, a critical appraisal of the informative value is provided, but a quantification of the relationship between, for example, a doubling of a proxy value and the change in population density does not take place and cannot take place for lack of a suitable framework or external data for calibration.

Archaeological data used to estimate population trends have the following characteristics, leading to these problems:

-   Limited: We have only incomplete data that can be used for these purposes, and they are usually very low in information value.
-   Unevenly distributed: Although there is a good data on settlement frequencies for some regions, and these are sometimes in very high temporal resolution, such favorable situations are very unevenly distributed over time and space.
-   Noisy: Often other influences, not related to population development, affect the characteristics of individual proxies. Preservation conditions, depositional behavior or social aspects should be mentioned here as an example.
-   Unreliable: Comprehensive information and derived data are influenced by research strategies, research history and financial capability of the individual research projects. Systematic distortions are possible or rather the rule.
-   Highly heterogeneous: The number of artefacts at a site, the number of houses, the demographic conditions in cemeteries, the estimates of human impact based on pollen analyses: All these are data that may serve our purposes, are available in completely different spatio-temporal scales, granularities and information values, and simply represent technically very different data formats and scales.
-   Indirect: It is actually always proxy data that are collected as substitute values for the information that is actually needed (i.e. population density or size). The transfer functions that link the collected data with the data to be obtained are unknown and are at best established by observing the (also ethnological) presence. However, since living conditions, circumstances and economic parameters differ greatly between the present time and prehistory, these transfer functions can only be evaluated as a first approximation.
-   Contradicting: Particularly when considering several proxies, differences in transfer functions and data quality and noisiness inevitably lead to contrasts. However, this usually only leads to the rejection of a proxy for a certain time period, without mutually supporting or contradictory information leading to a quantitative evaluation of the proxy and its uncertainties.

## Strengths of a hierarchical Bayesian approach

In recent years, a new method has been established in the field of demographic estimations. The Bayesian population forecast was developed to provide a solution to these problems, which also exist in demographic estimates with current data. In their book on the subject, Bryant and Zhang [@bryant2018] state that Bayesian data modeling can be used to deal with exactly those problems that also affect archaeological data. Especially for limited, unreliable and noisy data, Bayesian approaches are optimally suited. Various even contradictory data can be brought into a common framework and support each other. Similarly, unlikelihood and uncertainty of a model approach can be quantified. This is well-accepted in archaeological applications through the Bayesian calibration and modelling of stratigraphic conditions and radiometric data. Furthermore, spatially and temporally incomplete data can be taken into account: Where these data are missing, the uncertainty automatically increases, but this does not prevent general modelling and estimation. Finally, hierarchically structured model suites, in which sub-models are created for the utilisation of individual proxies, can also be used to estimate transfer functions between individual proxies and the value to be modelled, thanks to the interaction of a large number of data sources and evidence.

This modeling technique can thus be used to combine different lines of evidence horizontally and vertically and in this way combine their results into a common conclusion and estimate, which at the same time includes an estimation of their reliability: if the data contradict each other, the overall reliability will be lower. If they support each other, the confidence interval will be smaller. And if there is no systematic bias that affects all data sources to the same extent, it should be possible to arrive at the most reliable estimate possible through the most heterogeneous set of data sources.

Bayesian modelling techniques have also been used recently as a tool for hypothesis testing of demographic trends or underlying models based on ^14^C data. Most notably are the recently published papers by Crema and Shoda ([@crema2021a]). However, the approach taken there is clearly different from the one presented in this paper.In these analyses, deductive models are generated and their plausibility is tested on the basis of ^14^C data only. This is a clear step in the direction of a model-based, and thus scientific, analysis. However, the use of only one proxy, and its use exclusively for testing hypotheses developed independently, creates problems comparable to those of inductive approaches used so far: due to the lack of a combination with other indicators, one is limited to the problems and conditions of sum calibration as a tool. Furthermore, this approach loses significant potential information that would be gained by a direct evaluation of the time series. Thus, the credibility of a model can only be checked as a whole, without the dynamic developments that can arise in the course of demographic processes being represented. Therefore, we would like to better exploit the capabilities of Bayesian hierarchical models through a combination of inductive data analysis and model-passed data integration of different proxies.

This contribution now represents an attempt to make these techniques usable for archaeological reconstructions. In addition to a presentation of the basics and possible procedures, we want to show in the following, in a reproducible and practical form using a case study, how Bayesian methods can also make a decisive contribution to a better assessment of population development. These assessments are crucial for the reconstruction of the human past, even in for periods for which we only have very patchy, noisy and unreliable data.

# Materials and Methods

## Background

The basis of Bayesian statistics is the premise that there is always some prior assumption about the probability of an event, even if it may be completely vague. This assumption is then adjusted by observing the data. The concept of probability is not derived from theoretical, infinitely repeatable random experiments or distributions, but from the direct confrontation of the pre-assumptions (priors) with the available data. This involves checking how credible these prior assumptions are with regard to the available data [likelihood; see also @bryant2018, 66]. A small amount of data leads to a broad probability distribution that is not strongly localised and restricted. The intuitive procedure is thereby the shifting of credibility through evidence.

At the heart of all Bayesian statistics is therefore the concept of updating a given prior assumption with new data and expressing this in probabilities [cf. also @kruschke2015, especially 15--25]. Our assumptions about the demographic development of the past must naturally be very conservative. In the logic of the basic Bayesian equation, these assumptions represent the prior (probability). In conjunction with the data that are included as the likelihood of the prior, a posterior (probability) results, which represents the Bayesian learning from data. It is also in the nature of the approach that in real applications there is no point prediction, but in most cases a probability distribution for the prediction. Thus, we simultaneously obtained a result and an estimate of the confidence intervals, or better, the credibility interval given the data.

This Bayesian learning is iterative and sequential, so that the result of one Bayesian inference can form the prior of another, i.e. it is an additive process [@kruschke2015, 17]. Moreover, at the conceptual level, this allows different sources of information to be combined [@bryant2018, 219--224]. This enables that they can be mapped to the same set and the same real-world domain, e.g. probabilities. This fact has long been exploited by archaeology in using stratigraphic information to make radiometric dating more accurate [@ramsey1995]. ^14^C dates and stratigraphy are something completely different, but both can be mapped to the probability of older and younger and combined in this way. The same is also feasible when it comes to the probability of population sizes or population densities or their derived dynamics.

A further characteristic of Bayesian modeling is that, due to the fact that results of a Bayesian inference can be regarded as a prior for the next one, a hierarchical formulation of problem domains is possible. Parameters that are necessary for an estimation, such as the relationship of population density to the clearing signal in pollen data, need not be specified explicitly, but can be given by probability distributions and then estimated in the model itself [@bryant2018, 186]. The more data available, the more degrees of freedom can be estimated with a reasonable loss of confidence or a reasonable width of credibility intervals [@kruschke2015, 112]. For the estimation of these parameters, in turn, submodels have to be created which describe the relationship of the data to the characteristics of the parameter. This can be carried out over several levels, depending on necessity [@kruschke2015, 221--222].

## Data/Proxies

The Swiss Plateau (Figure \@ref(fig:mapswissplateau)) covers about one third of Switzerland's territory and comprises the partly flat but largely hilly area between the Jura Mountains and the Alps. Its average altitude above sea level is between 400 and 600 m. Due to its situation as a basin, it represents a favourable area and is by far the most densely populated region of the Switzerland today. Although the Swiss Plateau is a basin, depending on the region it has a very diverse natural landscape. The Swiss Plateau was shaped by glaciers during the Ice Age, which led to the many lakes that characterise the landscape today, and which provide excellent preservation conditions for the numerous Neolithic and Bronze Age lakeside settlements, as well as a rich pool of sources for vegetation reconstructions by means of pollen analyses. A very active and efficient archaeological research and heritage management also offers a rich repository of archaeological data, known sites and ^14^C data.

In principle, a large number of different data sources can be integrated into the overall model as observations, provided that these observations a) can be understood as dependent on the population density in the past, and b) a model-like description of this dependence can be created. A non-exhaustive list can be found in the following table \@ref(tab:tableproxies). For our analysis of the Swiss Plateau we used an open land indicator, an aoristic sum of sites based on typological dating, a sum calibration and the number of dendro dated lakeshore settlements in the Three Lakes area. The individual proxies will be discussed in more detail below.

| Proxies                                        |
|------------------------------------------------|
| Expert estimates                           |
| Ethnographic Analogies                         |
| Carrying Capacity                              |
| Economic modelling                             |
| Extrapolation of buried individuals            |
| Burial anthropology                            |
| Settlement data, number of houses              |
| Settlement data, settlement size               |
| **Aoristic analysis**                          |
| **Dendro dates**                               |
| Amount of archaeological objects               |
| **Radiocarbon sum calibration**                |
| Estimates based on specific object types       |
| **Human impact from pollen or colluvial data** |
| aDNA based estimates                           |
| ...                                            |

: (#tab:tableproxies) A incomplete list of possible observation that can be linked to population developments in the past. Proxies used in this study are highlighted.

```{r mapswissplateau, fig.cap="Location and extent of the Swiss Plateau."}
main_map <- my_basemap +
  geom_sf(data = fc, inherit.aes = FALSE, alpha = 0.33, fill="darkred", color=NA) +
  blank() +
  my_scalebar

inset_map <- europe_overview_map +
  geom_sf(data = switzerland_bb, fill = NA, color = "darkred", inherit.aes = FALSE) +
  blank()

ggdraw() +
  draw_plot(main_map) +
  draw_plot(inset_map, x = 0, y = 0.7, width = 0.3, height = 0.3)
```

### Dendro Dated Lakeshore Settlements

With a mild climate compared to the Jura and the Alps and a dense network of water bodies, the Swiss Plateau is a favourable area for human settlement in Switzerland. From the Neolithic onwards, settlement areas were concentrated along its rivers and lakes [@christianlüthi2009]. Thus, our working region offers on the one hand excellent data for demographic estimation, but on the other hand poses very specific problems for such an undertaking. If we have high-resolution information on the temporal sequence of individual settlements at the lakeside settlements by means of dendro data, this also might cause a research problem with regard to the ^14^C data often used as a proxy.

```{r echo=FALSE}
dendro <- read.csv(file.path(rstudioapi::getActiveProject(), "data", "raw_data", "dendro_years.csv"))
```

The dataset we use for the number of dendrodated wetland settlements in the Three Lakes region was collected by Julian Laabs for his PhD thesis [@laabs2019]. Details on the creation of this data series will be published at the referenced location. The time series used here runs from `r abs(min(dendro$date))` to `r abs(max(dendro$date))` BCE, and contains the number of chronologically registered fell phases at individual settlements. This results in a time series that reflects the settlement of the lakeshores in the Neolithic and Bronze Age periods.

### ^14^C sum calibration

```{r echo=FALSE}
c14 <- read.csv(file.path(rstudioapi::getActiveProject(), "data", "raw_data", "14c_swiss_plateau.csv"))
```

The dataset for the ^14^C sum calibration primarily consists of data from the XRONOS database, supplemented by dates from the unpublished PhD thesis of Julian Laabs [@laabs2019] and the data collection of [@martínez-grau2021]. It contains a total of `r nrow(c14)` single ^14^C data from `r length(unique(c14$site))` sites. The dates were selected so that their distribution area coincides with the catchment area of the pollen proxy (see also Figure \@ref(fig:c14map)). The dates in the dataset range in ^14^C years from `r max(c14$bp)` to `r min(c14$bp)` BP uncal. This time window extends beyond the study horizon in order to minimise boundary effects, which, however, cannot be completely avoided due to the Hallstatt plateau. But it was precisely to reduce its influence that the time window of the investigation was chosen to be 7000 -- 1000 BCE. The upper limit is set by the post-glacial changes in the pollen spectra, which are hardly associated with human influence before 7000 in the working area, and which would consequently distort the openness indicator. The lower limit is defined by the Hallstatt Plateau (with buffer), which would have direct and indirect research influences on the ^14^C proxy.

We binned the data at site levels to obtain a temporally dispersed count and thus an expected value of contemporaneous ^14^C dated sites. For the creation of the cumulative calibration, the corresponding functions of the R package rcarbon [@crema2021] were used in the standard settings.

```{r c14map, fig.cap="The location of the ^14^C dated sites in the dataset."}
my_basemap + geom_point(data = c14, aes(x = lng, y = lat), color = "darkred", alpha = 0.75) + blank() + my_scalebar
```

```{r compdendroc14, fig.cap="Comparison of the scaled (non-centered z-transformed) number of ^14^C and dendrodated sites over time in the dataset used."}
joined_data <- read.csv(file.path(rstudioapi::getActiveProject(), "data", "preprocessed_data", "all_proxies.csv"))

ggplot(joined_data) + geom_line(aes(x=(1950-age)*-1, y=scale(sumcal, center = F), color = "darkred"), alpha = 0.75)  + geom_line(aes(x=(1950-age)*-1, y=scale(dendro, center = F), color = "darkblue"), alpha = 0.75) +
  theme_minimal() + scale_x_reverse() +
    labs(x = "years BCE",
         y = "scaled values",
         color = "Legend") + 
    scale_color_manual(values = c("darkblue", "darkred"), labels = c("dendro dated", "14C dated"))+
  theme(legend.position="bottom")
```

We can now compare these two data sets (Figure \@ref(fig:compdendroc14)). In fact, there is a not uninteresting fit between the two data series. However, it must be assumed that the two dating methods, even if they would contradict each other, actually complement each other, and thus allow a better overall unified picture of the actual settlement density than each of the individual proxies would allow on their own.

### Aoristic Sum

```{r echo=FALSE}
aorist <- read.csv(file.path(rstudioapi::getActiveProject(), "data", "raw_data", "site_data_fuzzy.csv"))
aorist_latlng_coords <- st_as_sf(aorist,coords = c("LV03_Rechtswert_fuzzy", "LV03_Hochwert_fuzzy"),crs = 21781) %>% sf::st_transform(
  4326
) %>% st_coordinates() %>% as.data.frame()
```

<!-- JR: At the resolution the figure is rendered at, I don't think the 1 km jitter is perceptible. MH: But in the data table that accompanies the repository, they would be available -->

```{r aoristmap, fig.cap="The location of the sites from the find reports of the cantonal archaeology (heritage management). The locations are fuzzified by ~1km."}
my_basemap + geom_point(data = aorist_latlng_coords, aes(x = X, y = Y), color = "black", alpha = 0.75) + blank() + my_scalebar
```

In order to add another indicator of archaeological evidence of occupation, we have included the data of the Cantonal Archaeology (Figure \@ref(fig:aoristmap)), and thus the Heritage Management, which are primarily derived from scattered surface finds, and which often have a low dating accuracy. This information is incorporated into our model as a typologically dated aorist time series. The dating accuracy is only in the range of archaeological periods, but the advantage is that we are not bound to the conditions and problems of radiocarbon dating and thus methodological issues of sum calibration can be avoided. Much more, these data provide an independent indicator with regard to the methodology of the ^14^C data, even if they are influenced by similar transmission filters and archaeological conditions as the evaluation of ^14^C data. Data from `r nrow(aorist)` sites were included in the aoristic sum, which is a very rough indicator due to the low dating accuracy in archaeological phases, but which nevertheless has an important role in the normalisation of the data due to its independence from calibration effects.

```{r aoristcurve, fig.cap="Aoristic sum of the archaeological sites used in this analysis."}
ggplot(joined_data) + geom_line(aes(x=(1950-age)*-1, y=aoristic_sum, color = "darkgray"), alpha = 0.75) +
  theme_minimal()  +  scale_x_reverse() +
    labs(x = "years BCE",
         y = "aoristic sum",
         color = "Legend") + 
    scale_color_manual(values = "darkgray", labels = "Aoristic Sum")+
  theme(legend.position="bottom")
```

### Openness Indicator

The natural conditions provided by the many lakes enable not only highly precise dating of archaeological sites, but also a very dense network of pollen analysis. We make use of this fact by generating a supra-regional openness indicator for the vegetation from the pollen data (Figure \@ref(fig:pollensites)). This indicator has the specific advantage that it is not dependent on preservation conditions, as archaeological indicators are. This makes it particularly valuable for indicating or compensating for systematic distortions that result from temporally specific settlement patterns and archaeological preservation conditions.

```{r pollensites, fig.cap="Location of the pollen profiles used for the openness indicator."}
pollen_sites <- read.csv(file.path(rstudioapi::getActiveProject(), "data", "raw_data", "pollen_locations.csv"))

my_basemap +
  geom_point(data = pollen_sites, aes(x = lng, y = lat), color = "darkgreen", alpha = 0.75) + blank() +
  geom_text_repel(data = pollen_sites, aes(x = lng, y = lat, label = name), alpha = 0.75, nudge_x = 0.2, nudge_y=0.02) +
  my_scalebar
```

The utilisation of this proxy is based on the assumption that the higher the population density in an area, the greater the human influence on the natural environment [@lechterbeck2014], and that the agricultural space of an area is closely related to the population density [@zimmermann2004]. Evidence of land clearing in pollen diagrams can therefore provide further indications of population dynamics where humans are the main driver of this process, which is the case in much of Europe. The exact methodological procedure for obtaining this proxy from several, in our case 5, different pollen diagrams of sites mainly in the hinterland of the large Alpine lakes can be found in a previous publication [@heitz2021]. The technical procedure is also documented in the accompanying R-script. The percentage pollen data based on a pollen sum of all terrestrial taxa of the individual sites were combined into one data set by means of a principal component analysis (Figure \@ref(fig:pollenproxy)). Only terrestrial pollen taxa with a frequency of more than 1/3 and, if present, with an average frequency of at least more than 0.1% were selected to reduce potential disturbance by rare species. Cereal pollen was explicitly retained as an important anthropogenic indicator. As each sample is absolutely dated, the data on the x-axis can be plotted against the openness value on the y-axis to obtain a time series time series for land clearing.

```{r pollenproxy, fig.cap="Value on the first dimension of the PCA against dating of the samples for the individual pollen profiles and their combined average value as the openness indicator."}
knitr::include_graphics(file.path(rstudioapi::getActiveProject(), "figures", "pollenproxy.pdf"))
```

Unfortunately, in the Swiss Plateau there is no data from burials that could be usefully incorporated into such this model due to the absence of regular burials for larger parts of prehistory. Formation conditions, in this case most likely C-transformation [@schiffer1987], prevent their use in this study. Nevertheless, we see a very high potential for other regions in the integration of demographic indicators from burial data in order to enlarge the canon of methods and the range of proxies.

The values of the sum calibration, the openness index and the dendrodated settlements were smoothed by means of a moving average with a window of 50 years. Since the aoristic sum already had a very coarse temporal resolution, this was not applied for this measure. The range of the smoothing window corresponded to the sample interval, with which a unified resolution of 50 years was obtained for all proxies as time slices for the model. In addition, all data were restricted to the window of observation of 7000--1000 BCE.

In our following modelling (see also \@ref(observational-model)) we now consider all these proxies as determinants for the change in the number of settlements. In the model, we therefore change the causality and shift the measurement error, which is certainly inherent in each of these indicators, to the process model, in which a Poisson process describes the number of simultaneous settlements. In doing so, we establish a likelihood that indicates how credible the data are, given the model.

## Process Model

A special class of Bayesian hierarchical models are so-called State Space Models (also known as Hidden Markov Models). These are specifically designed for time series and follow two basic principles. First, a hidden or latent process is assumed, which represents the state of the variable of interest $x_t$ at all times. For the course of the states of this variable over time, it is assumed that every state of variable x in the future, as well as in the past, is bound by a Markov process to the state of variable $x$ at time $t$. At the same time, it is assumed that certain observations, represented in variable y, are dependent on the state of variable $x$ at time $t$. This means that there is a relationship between the variable $x$ and the state of variable y over time. This implies that a relationship between the individual states of variable $y$ is generated over time via the hidden variable $x$, which itself is not observable.

This basic structure of the model makes it particularly useful and suitable for the purpose of demographic reconstruction using archaeological, but also other data, which depend on population density in the past. This population density itself is not accessible or measurable by our means. All we have at our disposal are observations derived by unknown transfer functions. These can be of very different natures, such as number of archaeologically observable settlements, or other effects that can be observed through time series, and which are influenced by population in a given area. In our example, these are the openness indicators from pollen data, which we can interpret primarily in terms of human influence and its intensity. On a more abstract level, we could also include expert estimates, as these also happen on (often unspecified) bases that are at least indirectly influenced by past population density.

The overall model for the estimation of demographic developments is broken down into several hierarchically interconnected individual elements in accordance with the basic structure of a state space model. The basis is a process model that represents the demographic development itself in terms of a structure model, without this model already being explicitly configured with data.

In this process model we assume that the latent variable number of sites is strongly autocorrelated across different time periods, i.e. the number of sites in 3000 BCE is strongly conditioned by the number of sites in 3050 BCE, and so on. In principle, one can represent a population development in such a way that the population at time $t$ results from the population at time $t-1$ times a parameter $\lambda$, which represents the population change at this time. This results in the following very simple formula:

$$
N_t = N_{t-1} * \lambda_t
$$

A distribution that is particularly suitable for modelling frequencies is the Poisson distribution. It is a univariate discrete probability distribution that can be used to model the number of events that occur independently of each other at a constant mean rate in a fixed time interval or spatial area. It is determined by a real parameter $\lambda$ \>0, which describes the expected value and simultaneously the variance of the distribution. Thus, the relationship shown above can also be rearranged as follows:

$$
\begin{aligned}
N_t &\sim dpois(\lambda_t) \\
\lambda_t &= N_t
\end{aligned}
$$ If we now have information about the change in population development (the proxies), we can use this to enter it into the model via a change in $\lambda$. This is done in the form of a regression: For all proxy values as a vector of independent variables $x \in R^n$, $R^n$ as an n-dimensional Euclidean space, described in this case by the n dimensions of the n variables, then the model takes the form

$$
log({E} (Y\mid x))=\alpha + \beta' x
$$

Logarithmisation as a link function ensures that $\lambda$, which must always be positive for a Poisson process, can also be described by variables (proxies) that range in the space of real numbers and can therefore also be negative. $\beta$ can serve here as a slope factor, just as in a normal linear regression. In our case, it functions as a scaling factor for the individual proxies. $\alpha$, on the other hand, is to be understood as an intercept. If there were no change due to the variables, the regression would fall back to this value. This corresponds to the desired behaviour. In the case of population trends, the intercept would be equal to the value of the population in the previous time period, plus or minus the changes resulting from the variables. If there is no change from these variables, then $\lambda$, and thus the expected value for the current time step would be equal to that from the previous time step:

$$
log(\lambda_t) = log(N_{t-1}) + \sum_{i=1}^n \beta_i x_{t,i}
$$

Since $\lambda$ and $N$ are essentially in the same range (e.g. if $lambda=1$, the expected value for $N$ would also be 1), $N_{t-1}$ must also be logarithmised in the above formula to obtain the congruence of both values. The values for population size $N_t$ as well as for population change $\lambda_t$ are time-dependent. At each individual point in time in the time series, these variables can or will also take on different values. However, we can narrow down the structure of population change even further. We can assume that, considered overall over time, the population change the population change will not exceed certain limits over the entire time span, without it being possible here to specify this already.

Thus, we can define the limits, the $max\_change\_rate$<!-- JR: no Greek letter? :( MH: I could invent one, if you like? ;-) --> as an time independent variable, again without already specifying them with fixed values at this stage. The estimation of these parameters for the entire model, as well as the estimation of the respective population change per time section, results from the modelling and the interaction with the data, respectively. Overall, this represents a hierarchical model that can be noted as follows:

$$
\begin{aligned}
max\_growth\_rate &\sim dgamma(shape = 5, scale=0.05) \\
N_t/N_{t-1} &< (max\_growth\_rate + 1) \\
N_{t-1}/N_t &< (max\_growth\_rate + 1)
\end{aligned}
$$

The gamma distribution used centres the probability in a range $[0,1[$, adding 1 makes this range $[1-2[$. This prevents the number of sites from explosively increasing between two time periods, which would lead to problems for the convergence of the model. The interaction of these parameters results in the following prior probability distribution for $\lambda$ and thus the growth (or change) rate of the population:

## Observational Model

During the development of the overall model, we abandoned the implementation of dedicated observation models adapted to the conditions of the individual proxies and their generating processes. In previous implementations, the underdetermination by the currently usable data of the model with many degrees of freedom led to equifinality of the solutions and, thus, to a high path dependency of the individual model runs. Therefore, it was almost impossible to achieve convergence of the overall model. Nevertheless, we believe that for a future application of the model with more data, a larger geographical coverage and especially a regionalised approach with information transfer by means of partial pooling, this more specific approach will be feasible and a very useful approach.

The implementation we introduce in this paper represents a Poisson regression where the proxies are used to inform the change in the number of settlements from time step to time step. For this purpose, the individual proxies were z-normalised. The absolute differences from one time step to another were then computed from the resulting time series. Thus, if the value of the proxy increases, this results in a positive difference from the previous time step, and vice versa.

$$
\begin{aligned}
z_t &= \frac{x_t - \bar{x}} {\sigma_x}\ |\ \sigma_x := Standard\ Deviation  \\
\delta z{_t} &= z_t - z_{t-1}
\end{aligned}
$$

The sum of the resulting differences between the time steps, together with the settlement number of the previous step as the expected value, then forms $\lambda_t$ as the expected value for the settlement number of the current time step.

$$
log(\lambda_t) = log(N_{t-1}) + \sum_{i=1}^n \beta_i \delta z_{i, t}
$$

Here, $\beta_i$ is a scaling factor that represents the influence of the respective proxy. It is a confidence value of the model for the respective proxy, so that the sum of all $\beta_i$ results in 1.

$$
\sum_{i=1}^n \beta_i = 1
$$

A probability distribution that can be used for this purpose in a hierarchical Bayesian model is the Dirichlet distribution, which is a multivariate generalization of the beta distribution, commonly used as prior distributions in Bayesian statistics. Its density function gives the probabilities of $i$ different exclusive events. It has a parameter vector $\alpha = (\alpha_1, ..., \alpha_i)\ |\ (\alpha_1, ..., \alpha_i) > 0$, for which we have chosen a weakly informative log-normal prior. The priors for the log-normal distribution in turn come from a weakly informative exponential distribution for the mean and a log-nomal distribution with $\mu$ of 1 and $\sigma_{log}$ of 0.1:

$$
\begin{aligned}
\beta_i &\sim Dir(\alpha_{1-i}) \\
\alpha_i &\sim LogNormal(\mu_{alpha_i}, \sigma_{alpha_i}) \\
\mu_{alpha_i} &\sim Exp(1) \\
\sigma_{alpha_i} &\sim LogNormal(1,0.1)
\end{aligned}
$$

As an intuition, this means that we consider the sum of the proxies as determinant for the number of settlements. The estimation therefore assumes that all proxies together give the best possible estimation result for contemporaneous sites at time t, whereby the share of each individual proxy is considered variable and is estimated within the model. This share is recorded within the model as the parameter p.

The error value is represented by the Poisson process in the process model, rather than directly as an estimation error for the individual proxies. Thus, our model does not correspond to a classical state space model, where the measured values are each considered to be error-prone. In the implementation, the model finds the best possible combination or compromise between the individual proxies to describe a settlement dynamic that is given by them. In addition, the number of sites is converted into population density using some (certainly debatable) parameters that we have defined but which are only scaling factors for the intermediate value of number of settlements. For this, we assume that each site represents a number of people that is poisson distributed around the value 50. The number 50 represents a compromise, as both Mesolithic and Neolithic and Bronze Age settlement communities need to be represented. By means of a data series, which would represent an evidence-based estimate of the temporal development of settlement sizes, this specification could be made based on data. From the number of sites and the mean number of individuals represented in each case, a population density can be calculated using the area of the Swiss Plateau (12649 km²). The estimated result of the model is thus comparable with estimates from other sources or the literature.

```{r expertestimations, fig.cap="Expert estimations for the population density on the Swiss Plateau from different authors."}
expert_estimations <- read.csv(file.path(rstudioapi::getActiveProject(), "data", "raw_data", "estimates.csv"))

ggplot(expert_estimations) + geom_segment(aes(x=from * -1, xend = to * -1, y = estimate, yend = estimate, color = source)) + xlab("cal BCE") + ylab("Density p/km²") + theme_minimal() + scale_x_reverse()
```

<!-- JR: I really like the idea of discussing "dead ends" in the model design process – very modern and reflexive! Maybe hightlight these in a section of their own? MH: I do not feel that this part would provide enough material to justify an own section. -->

In earlier implementations, expert estimates were also integrated into the model. However, since these are highly contradictory for the working area (Figure \@ref(fig:expertestimations), therefore have little influence on the model, and also lead to a significantly longer runtime, we have refrained from doing so in the current implementation. For future applications of the model with a larger geographical range and thus a higher information density, however, this would again be a factor that could be integrated in a useful way.

## Model fitting

The model was fitted using the R package nimble (version 0.11.1, R version 4.1.3). For this purpose, 4 chains were run in parallel. Achieving and ensuring convergence and sufficient effective samples (10000) for a reliable assessment of the highest posterior density interval was carried out in steps.

In a first run, the model was initialised for each chain and run for 100000 iterations (with a thinning of 10). The computer used (Linux, Intel(R) Xeon(R) CPU E3-1240 v5 \@ 3.50GHz, 4 cores, 8 threads) needed approx. 1 min for this.

In a second step, the run was extended until convergence could be determined using Gelman and Rubin's convergence diagnostic, the criterion being that a potential scale reduction factor of less than 1.1 was achieved for all monitored variables. Convergence occurred after about 30 seconds.

Due to the high correlation of the parameters and thus a low sampling efficiency, the collection of at least 10,000 effective samples for all parameters in the third step took about 5 hours.

For the fitting process, a starting value of 5 p/km² for a population density of the Late Bronze Age (1000 BCE) was taken from the literature, which may represent a general average value for all prehistoric population estimates [@nikulka2016, 258]. For the model, this was set as the mean of a normal distribution with a standard deviation of 0.5, which should give enough leeway for deviations resulting from the data. Nevertheless, especially the late part of the reconstruction is of course clearly influenced by this predefined value.

For the traceplots and the prior-posterior overlap as well as the density functions of the posterior samples of the individual parameters, please refer to the supplementary material.

```{r, results='hide'}
x <- 1:100
```

```{r dpois50, results='hide'}
pdf(file = file.path(plot_path, "dpois50.pdf"), width = 2, height = 2)
oldmar <- par(mar = c(2, 2, 2, 2))
plot(x,dpois(x,50), type="l", ann = FALSE, axes = FALSE, lwd=4, col = "skyblue");axis(1)
par(oldmar)
dev.off()
```

```{r dgammamaxgrowthrate, results='hide'}
pdf(file = file.path(plot_path, "dgammamaxgrowthrate.pdf"), width = 2, height = 2)
oldmar <- par(mar = c(2, 2, 2, 2))
plot(x/50+1,dgamma(x/50,shape = 5, scale = 0.05), type="l", ann = FALSE, axes = FALSE, lwd=4, col = "skyblue");axis(1)
par(oldmar)
dev.off()
```

```{r mualpha, results='hide'}
pdf(file = file.path(plot_path, "mualpha.pdf"), width = 2, height = 2)
oldmar <- par(mar = c(2, 2, 2, 2))
plot(x/20,dlnorm(x/20,1,0.1), type="l", ann = FALSE, axes = FALSE, lwd=4, col = "skyblue");axis(1)
par(oldmar)
dev.off()
```

```{r aalpha, results='hide'}
pdf(file = file.path(plot_path, "aalpha.pdf"), width = 2, height = 2)
oldmar <- par(mar = c(2, 2, 2, 2))
plot(x/10,dexp(x/10,1), type="l", ann = FALSE, axes = FALSE, lwd=4, col = "skyblue");axis(1)
par(oldmar)
dev.off()
```

```{r alpha, cache=TRUE, results='hide'}
n_sample <- 1000000
mu_a <- rlnorm(n_sample,1,0.1)
a_a <- rexp(n_sample,1)
alpha <- rlnorm(n_sample,mu_a,a_a)
pdf(file = file.path(plot_path, "alpha.pdf"), width = 2, height = 2)
oldmar <- par(mar = c(2, 2, 2, 2))
plot(density(alpha, from=0,to=100), type="l", ann = FALSE, axes = FALSE, lwd=4, col = "skyblue");axis(1)
par(oldmar)
dev.off()
```

```{r p, cache=TRUE, results='hide'}
p <- matrix(nrow = 10000, ncol = 4)
for (i in 1:nrow(p)) {
 p[i,] <- nimble::rdirch(1, sample(alpha,4))
}
pdf(file = file.path(plot_path, "p.pdf"), width = 2.5, height = 2.5)
oldmar <- par(mar = c(2, 2, 2, 2))
par(mfrow=c(2,2))
plot(density(p[,1]), type="l", ann = FALSE, axes = FALSE, lwd=4, col = "skyblue");axis(1)
plot(density(p[,2]), type="l", ann = FALSE, axes = FALSE, lwd=4, col = "skyblue");axis(1)
plot(density(p[,3]), type="l", ann = FALSE, axes = FALSE, lwd=4, col = "skyblue");axis(1)
plot(density(p[,4]), type="l", ann = FALSE, axes = FALSE, lwd=4, col = "skyblue");axis(1)
par(oldmar)
par(mfrow=c(1,1))
dev.off()
```

| **Priors**           | **Value**                            | **Plot/Comment**                                                               |
|-----------------------------|--------------------------|-----------------|
| MeanSiteSize         | dpois(50)                            | ![MeanSiteSize](../figures/dpois50.pdf)                |
| max_growth_rate      | dgamma(shape = 5, scale=0.05) + 1    | ![max_growth_rate](../figures/dgammamaxgrowthrate.pdf) |
| mu_alpha             | dlnorm(1,sdlog=0.1)                  | ![mu_alpha](../figures/mualpha.pdf)                    |
| a_alpha              | dexp(1)                              | ![a_alpha](../figures/aalpha.pdf)                      |
| alpha                | dlnorm(mu_alpha[j],sdlog=a_alpha[j]) | ![alpha](../figures/alpha.pdf)                         |
| p                    | ddirch(alpha[1:4])                   | ![p](../figures/p.pdf)                                 |
| **Parameters**       |                                      |                                                                                |
| nEnd                 | 5                                    |                                                                                |
| AreaSwissPlateau     | 12649 km²                            |                                                                                |
| **Initial Values**   |                                      |                                                                                |
| lambda$_{1:nYears}$  | $log(1-10^{\frac{1}{nYears-1}})$     | exponential increase of the factor 10                                          |
| PopDens$_{1:nYears}$ | nEnd (=5)                            |                                                                                |
| nSites$_{1:nYears}$  | 50                                   |                                                                                |

: (#tab:priorsandparameters) Priors and fixed parameters used in the model.

# Results

<!-- JR: I really think we need a section evaluating and validating the model fit before we discuss its output. MH: that was the intention of the Model fitting section before, but I think this rather belongs to methods than to results!?-->

```{r popdensplot, fig.cap="The estimate of the population density resulting from the model and the four proxies, which are also plotted (scaled) for comparison."}
knitr::include_graphics(file.path(rstudioapi::getActiveProject(), "figures", "popdens_estimation.pdf"))
```

The population density estimate (Figure \@ref(fig:popdensplot)) ranges on average between 0.2 p/km² for the beginning of the estimate (6000 BCE) and 4.8 p/km² for the end of the estimate (1000 BCE), reaching a maximum of 6.5 p/km² for the time slice 1250 BCE. Thus, the estimate remains within the values that are also considered plausible by the expert estimates. Clear peaks are reached around 1250 BCE, as well as around 2750 BCE, which corresponds to the transition to the ceramic style in the Swiss Plateau influence by the Corded Ware.

```{r varcoeffplot, fig.cap="The variability of the estimate of the population density over time, with the estimate itself for reference."}
knitr::include_graphics(file.path(rstudioapi::getActiveProject(), "figures", "popvar_plot.pdf"))
```

Furthermore, it may be relevant to look at the temporal distribution of the variability in the estimate (Figure \@ref(fig:varcoeffplot)), to estimate at which time steps the model attempts a more accurate estimate and at which the uncertainty is greater due to e.g. contradictions in the indicators. The coefficient of variation is 0.13 for the beginning and 0.1 for the end of the estimate, the greatest variability is reached around 2150 BCE with 0.47. The beginning and end of the time series are relatively clearly determined. The end results from the a priori setting of the parameter, but also here as at the beginning of the series the proxies are very uniform, which explains the low variability. Overall, the variability is relatively uniform over the entire course of the estimation and averages over all time slices at 33% of the respective mean.

Within the model, the parameter p was estimated, which reflects the proportion of the individual proxies used in the estimation of the number of settlements. This parameter is variable, but has only a scaling influence on the final estimate of population density.

```{r pdistribution, fig.cap="Distribution of the influence ratio of the different proxies on the final estimation of the number of sites."}
knitr::include_graphics(file.path(rstudioapi::getActiveProject(), "figures", "p_estimation.pdf"))
```

By looking at the distribution of posterior samples for the share of each proxy (Figure \@ref(fig:pdistribution)), it is clear that the model weights the openness indicator the highest. The average is slightly above 60%. The next most important indicator is the sum calibration value, which has an average of about 20%. The aoristic sum is slightly above 10%, whereas the importance of the dendrodated settlements is below 10%. The reason for the latter is certainly that there are no lakeshore settlements over large areas of the time window, and therefore the proxy achieves a low confidence value in comparison with the other estimators. In the case of the aoristic sum, it is certainly the fact that it is flat over large sections and has little structure, making it difficult to relate to the other estimators. The sum calibration shows very strong short-term fluctuations, which are presumably at least partly due to the effects of the calibration curve, and which also make this proxy seem ill-suited to reliably represent a continuous population trend. Nevertheless, its fluctuations do have an impact on the resulting overall estimate of the development of the number of settlements, albeit to a lesser extent.

# Discussion

<!-- JR: I would split this into subsections evaluation a) the merits of the approach and b) the specific results for Switzerland. MH: I thought I already have done this, discussion more the results for switzerland, conclusion more the generel merrits!? -->

The ambition in the development of the model was originally to base it on a state-space representation of the demographic process itself, and then to integrate the existing proxies by means of an observation model to inform this process. In the course of developing the model, however, we had to realise that the existing data or the data used in the modelling process for the Swiss Plateau alone were not sufficient to adequately determine this process or to adequately fix the degrees of freedom resulting from the process and the transfer model. Therefore, in this implementation we have turned to the Poisson regression approach. In this version, the model represents the best possible combination of the indicators used to describe the development of the number of sites based on them.

In its present form, the reconstructed development already shows interesting aspects regarding the underlying proxies. Starting with the result of the sum calibration, which is currently the most frequently used indicator when attempting to reconstruct (relative) population changes. Its strong fluctuations are dampened by the connection of the other proxies, which is especially true for the first fluctuation around or shortly after 4000 BCE. The increase in the archaeological record with the process of Neolithisation is still clearly visible, but the curve of the overall estimate after the initial increase is much flatter than the sum calibration alone would suggest. The period between 3950 and 3700, which is contemporaneous with the first settlement cycle on the lakeshores of the Three Lakes area, in particular forms a noticeable plateau in the calibration curve, and this may lead to an overestimation of the ^14^C evidence. However, the effect of the calibration curve on the results of a cumulative calibration cannot yet be considered unambiguous. A second maximum, after 3000 BCE, is supported by the other proxies, and is consequently also much more clearly reflected in the overall estimate. Here, too, there is a smaller, albeit shorter plateau in the calibration curve, which is, however, much less pronounced than, for example, the one shortly before between 3350 and 3100. The rise towards the Middle and Late Bronze Age also receives support above all from the Aorist Sum, and therefore remains preserved in the combination of indicators. In this period, the calibration curve does not show any very clear, suspicious patterns.

Overall, the model trusts the cumulative calibration with an average of about 20% significantly less than the openness indicator with 60%. After the initial increase, which can easily be linked to the spread of agricultural production, the latter tends to fluctuate less and thus has a dampening effect on the overall estimate. Nevertheless, by all appearances, the general trends in the cumulative calibration are quite well reflected in the land opening, even if such eyeballing should be interpreted with caution. We would suggest that the changes within the Neolithic and Bronze Age are rather gradual compared to the much more severe qualitative shifts to a productive economy as well as to the later developments outside our window of observation, which are suggestive of a much more large-scale reshaping of the landscape. The model itself is designed rather conservatively by the limit of maximum growth, which is estimated within the model but influences it by its very presence. Therefore, this proxy corresponds better and more homogeneously to a smooth increase in the number of settlements than do the strong fluctuations in the ^14^C data set.

The aoristic sum remains relatively even over large areas of the observation window. It is not until the Middle and Late Bronze Age that their values rise significantly, which is also noticeable in the overall estimate by a clear increase. Overall, it remains to be seen to what extent a modelling of the taphonomic loss [@surovell2009] could be integrated. We have refrained from doing so in this first model, as this would have introduced further degrees of freedom - but we are aware that with a broader database this would be an interesting possibility, and that it would itself be a variable to be estimated, e.g. in connection with proxies that are not influenced by it (openness indicators, but also data from the demography of burial collectives). This would make it possible to estimate a value from original archaeological material independently of variables that have little to do with archaeological data, such as volcanic eruptions.

The number of simultaneously existing lakeshore settlements is a very limited temporal and spatial estimator, but extremely reliable. Its limitations are reflected in the low overall confidence of the model, since its value is 0 over long stretches, while other indicators suggest clearly different patterns. However, where it has information potential, such as around and shortly after 3800 BCE, 3200 BCE or especially around 2750 BCE, its fluctuations have a noticeable influence on the overall estimate. The peak around 1600 BCE also leaves a noticeable impact. This highlights another potential of our approach: where a proxy has little structure and thus little significance, or where its trends cannot be linked to other indicators, it consequently has little influence. For periods in which it can provide information, however, this will also feed into the overall model, despite a low overall confidence in the estimator.

In order to review the reconstruction against the background of established archaeological knowledge and narratives, we consider it useful to parallelise the archaeological phase boundaries with the changes in the estimate. It should be noted, however, that the estimate is not completely independent of the archaeological phases: due to the aoristic sum, which is itself strongly determined by this phase division, corresponding boundaries also influence the structure of the reconstruction. Nevertheless, it helps to check whether the estimate is in strong contradiction to the generally accepted picture or whether it is able to make a credible prediction within this context.

```{r popestwithphases, fig.cap="The estimate of the population density in relation to the chronology for the Swiss Plateau."}
PopDens_summary <- read.csv(
  file = normalizePath(
            file.path(here(), "data","preprocessed_data", "popdens_summary.csv"
                      )
            )
  )

chronology <- read.csv(
  file = normalizePath(
            file.path(here(), "data","raw_data", "chrono_rough.csv"
                      )
            ), row.names = 1
  )
chronology <- chronology[order(chronology$dat_begin),]
chronology$label <- factor(chronology$label, levels = chronology$label)

ggplot(PopDens_summary) +
  geom_line(aes(x = (1950-age)*-1, y = mean)) + 
  geom_line(aes(x = (1950-age)*-1, y = hpdi_l), lty=2) + 
  geom_line(aes(x = (1950-age)*-1, y = hpdi_u), lty=2) + 
    theme_minimal() + scale_x_reverse() +  
ylab("Scaled values for Proxies,\n P/km² for Estimation") +
  xlab("cal BCE") +
  geom_rect(data=chronology[-1,],aes(xmin=dat_begin*-1,ymin=0,xmax=dat_end*-1,ymax=Inf,fill=label),
                    alpha=0.5,inherit.aes=FALSE) +
    scale_fill_brewer(palette="Pastel1", guide = "none") +
  geom_text(data=chronology[-1,],
            aes(x=(dat_end - 100)* -1,y = 0.25,label=label),
            alpha=0.25,inherit.aes=FALSE, angle = 90, hjust = 0)
```

Figure \@ref(fig:popestwithphases) underlays the estimated development with the rough archaeological phases for the Swiss Plateau. The Older and Middle Neolithic phases are in fact hardly documented with known sites in Switzerland. Here we must assume a basically low level of settlement, probably mainly by hunter-gatherer groups. Isolated Neolithic sites of the LBK and later groups are known in the periphery of Switzerland, but they play a subordinate role. The Neolithic really began here in the so-called Upper Neolithic, connected with the cultural phenomena of Egolszwil and Cortaillod respectively Pfyn, as well as with the first lake shore settlements. In this period, we also see a clear increase in the estimated population in the model. In the transition to the Late Neolithic we know from the lakeshore settlements the so-called Horgen Gap. This is also visible as a slight decrease in the model. In another study [@heitz2021] we could show that this is in fact probably not a decline in population but rather a shift in settlement strategy. In the Late Neolithic, associated with the Horgen pottery, we then see a clear increase in the intensity of settlement, which reaches its peak and its break-off at the transition to the Corded Ware and thus to the Final Neolithic. In the second half of the Early Bronze Age, from which we again know lakeshore settlements, there is again a clear increase in population size according to the model, which continues until the Late Bronze Age. The general trends fit very well with the previous reconstructions of population development for Switzerland [see eg. @lechterbeck2014]. All in all, the estimate of the model corresponds to our expectations, although we must be aware that our expectations are not ground truth. Nevertheless, it can be said that the predictions of the model can be well integrated into the known narratives and make them more precise or provide them with a higher resolution, thus gaining credibility for the model predictions.

# Conclusion and Outlook

An important result of the current model is the estimation of absolute population numbers with uncertainty over time. This estimate can be used as a basis for further studies where relative measures of population development are not helpful, such as long-term land use studies where modelling of large-scale socio-ecological systems based on archaeological data becomes possible and does have to rely on using deductive, asynchronous population models (eg. carrying capacity or ethnographic analogues).

With this model and its predictions, we also believe we can demonstrate how it is possible to achieve a true multiproxy analysis of prehistoric demographic processes. Previous attempts in this regard have so far only achieved a juxtaposition of different indicators, without the possibility of truly linking them or assessing the confidence in the individual indicators. Also, in previous approaches it was not possible to specify a confidence interval for the estimates. Within a Bayesian framework with an observational model based on Poisson regression, all these demands can be met. We thus believe we can provide a better basis than before for estimating population processes and changes in the number of settlements.

The currently presented model is only the first step towards a more sophisticated approach. So far, we have trusted the individual indicators in their aggregation without modelling them with a measurement error, which they undoubtedly inherit. Moreover, our estimates are based on a limited number of indicators, almost all of which are subject to archaeological preservation conditions except for the openness indicator. Consequently, a transformation of the results into the range of absolute numbers for population density can only be achieved by means of predefined parameters for upper limit of population growth, settlement size and the initial value of the reconstruction. In the future application and refinement of the model, this must be overcome. For this purpose, further proxies independent of the immediate temporally changing conditions of the archaeological record have to be integrated. These can be data on settlement sizes, parameters for economic-ecological carrying capacity, demographic data from burial groups as well as palaeogenetic data on population sizes. Other possible indicators can still be identified.

These data are available to varying degrees in different regions. In the Swiss Plateau, for example, there are too few data on human remains over large areas of prehistory to be able to integrate them meaningfully into a model. Also, for different regions proxies used in this paper (eg. the palynological openness indicator) have to be adapted to fit the local conditions. By means of large-scale modelling, however, it would be possible to supplement gaps in the data of individual regions with regions in which these data are available by means of regionalisation and partial transfer of information (partial pooling). Such an extension would be the next logical step in the improvement of the model, to which we hope to be able to contribute a further study in the near future.

# Acknowledgements

Large parts of the data collection were conducted as part of the project 'Beyond lake settlements' in the doctoral thesis of Julian Laabs, funded by the SNF (project number 152862, PI Albert Hafner). Further data collection was done as part of the XRONOS project, also funded by the SNSF (project number 198153, PI Martin Hinz). The development of the openness index took place within the framework of the project Time and Temporality in Archaeology (project number 194326, PI Caroline Heitz) and was inspired by the cooperation within the project QuantHum (project 169371, PI Marco Conedera), both also funded by the SNSF. We also thank the Institute of Archaeological Sciences of the University of Bern for its support and faith in the outcome of our modelling project. Finally, we thank (already) the unknown reviewers for their helpful comments, which will certainly improve this manuscript significantly.

\newpage

# References

::: {#refs}
:::

\newpage

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r colophon, cache = FALSE}
# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```
